
start a new running with args: Namespace(add_doc_num=319927, batch_size=0, dataset_cache_dir='../../negs_tutorial_cache', dataset_script_dir='../data_scripts', doc_file_path='..//dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='..//dataset/encoded_docid/t5_atomic_top_300k.txt', epochs=10, learning_rate=0.001, load_ckpt='False', load_ckpt_path='./model/', log_path='..//logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', max_docid_length=1, max_seq_length=128, num_beams=10, operation='training', output_every_n_step=5000, per_gpu_batch_size=100, pretrain_model_path='..//transformer_models/t5-base', save_every_n_epoch=2, save_path='..//outputs/t5_128_10_top_300k_atomic_pretrain/', test_file_path='..//dataset/test_data_top_300k/', train_file_path='..//dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', use_docid_rank='False', use_origin_head='False', warmup_ratio=0)

start a new running with args: Namespace(add_doc_num=319927, batch_size=0, dataset_cache_dir='../../negs_tutorial_cache', dataset_script_dir='../data_scripts', doc_file_path='..//dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='..//dataset/encoded_docid/t5_atomic_top_300k.txt', epochs=10, learning_rate=0.001, load_ckpt='False', load_ckpt_path='./model/', log_path='..//logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', max_docid_length=1, max_seq_length=128, num_beams=10, operation='training', output_every_n_step=5000, per_gpu_batch_size=100, pretrain_model_path='..//transformer_models/t5-base', save_every_n_epoch=2, save_path='..//outputs/t5_128_10_top_300k_atomic_pretrain/', test_file_path='..//dataset/test_data_top_300k/', train_file_path='..//dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', use_docid_rank='False', use_origin_head='False', warmup_ratio=0)

start a new running with args: Namespace(add_doc_num=319927, batch_size=0, dataset_cache_dir='../../negs_tutorial_cache', dataset_script_dir='../data_scripts', doc_file_path='..//dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='..//dataset/encoded_docid/t5_atomic_top_300k.txt', epochs=10, learning_rate=0.001, load_ckpt='False', load_ckpt_path='./model/', log_path='..//logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', max_docid_length=1, max_seq_length=128, num_beams=10, operation='training', output_every_n_step=5000, per_gpu_batch_size=100, pretrain_model_path='..//transformer_models/t5-base', save_every_n_epoch=2, save_path='..//outputs/t5_128_10_top_300k_atomic_pretrain/', test_file_path='..//dataset/test_data_top_300k/', train_file_path='..//dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', use_docid_rank='False', use_origin_head='False', warmup_ratio=0)

start a new running with args: Namespace(add_doc_num=319927, batch_size=0, dataset_cache_dir='../../negs_tutorial_cache', dataset_script_dir='../data_scripts', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', epochs=10, learning_rate=0.001, load_ckpt='False', load_ckpt_path='./model/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', max_docid_length=1, max_seq_length=128, num_beams=10, operation='training', output_every_n_step=5000, per_gpu_batch_size=100, pretrain_model_path='../transformer_models/t5-base', save_every_n_epoch=2, save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', test_file_path='../dataset/test_data_top_300k/', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', use_docid_rank='False', use_origin_head='False', warmup_ratio=0)

start a new running with args: Namespace(add_doc_num=319927, batch_size=0, dataset_cache_dir='../../negs_tutorial_cache', dataset_script_dir='../data_scripts', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', epochs=10, learning_rate=0.001, load_ckpt='False', load_ckpt_path='./model/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', max_docid_length=1, max_seq_length=128, num_beams=10, operation='training', output_every_n_step=5000, per_gpu_batch_size=100, pretrain_model_path='../transformer_models/t5-base', save_every_n_epoch=2, save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', test_file_path='../dataset/test_data_top_300k/', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', use_docid_rank='False', use_origin_head='False', warmup_ratio=0)

start a new running with args: Namespace(add_doc_num=319927, batch_size=0, dataset_cache_dir='../../negs_tutorial_cache', dataset_script_dir='../data_scripts', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', epochs=10, learning_rate=0.001, load_ckpt='False', load_ckpt_path='./model/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', max_docid_length=1, max_seq_length=128, num_beams=10, operation='training', output_every_n_step=5000, per_gpu_batch_size=100, pretrain_model_path='../transformer_models/t5-base', save_every_n_epoch=2, save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', test_file_path='../dataset/test_data_top_300k/', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', use_docid_rank='False', use_origin_head='False', warmup_ratio=0)

start a new running with args: Namespace(add_doc_num=319927, batch_size=0, dataset_cache_dir='../../negs_tutorial_cache', dataset_script_dir='../data_scripts', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', epochs=10, learning_rate=0.001, load_ckpt='False', load_ckpt_path='./model/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', max_docid_length=1, max_seq_length=128, num_beams=10, operation='training', output_every_n_step=5000, per_gpu_batch_size=100, pretrain_model_path='../transformer_models/t5-base', save_every_n_epoch=2, save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', test_file_path='../dataset/test_data_top_300k/', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', use_docid_rank='False', use_origin_head='False', warmup_ratio=0)

start a new running with args: Namespace(epochs=10, per_gpu_batch_size=100, learning_rate=0.001, warmup_ratio=0, output_every_n_step=5000, save_every_n_epoch=2, operation='training', use_docid_rank='False', load_ckpt='False', save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', test_file_path='../dataset/test_data_top_300k/', pretrain_model_path='../transformer_models/t5-base', load_ckpt_path='./model/', dataset_script_dir='../data_scripts', dataset_cache_dir='../../negs_tutorial_cache', add_doc_num=319927, max_seq_length=128, max_docid_length=1, use_origin_head='False', num_beams=10, batch_size=200)

start a new running with args: Namespace(epochs=10, per_gpu_batch_size=100, learning_rate=0.001, warmup_ratio=0, output_every_n_step=5000, save_every_n_epoch=2, operation='training', use_docid_rank='False', load_ckpt='False', save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', test_file_path='../dataset/test_data_top_300k/', pretrain_model_path='../transformer_models/t5-base', load_ckpt_path='./model/', dataset_script_dir='../data_scripts', dataset_cache_dir='../../negs_tutorial_cache', add_doc_num=319927, max_seq_length=128, max_docid_length=1, use_origin_head='False', num_beams=10, batch_size=200)
Epoch 1/10

start a new running with args: Namespace(epochs=10, per_gpu_batch_size=100, learning_rate=0.001, warmup_ratio=0, output_every_n_step=5000, save_every_n_epoch=2, operation='training', use_docid_rank='False', load_ckpt='False', save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', test_file_path='../dataset/test_data_top_300k/', pretrain_model_path='../transformer_models/t5-base', load_ckpt_path='./model/', dataset_script_dir='../data_scripts', dataset_cache_dir='../../negs_tutorial_cache', add_doc_num=319927, max_seq_length=128, max_docid_length=1, use_origin_head='False', num_beams=10, batch_size=200)
Epoch 1/10

start a new running with args: Namespace(epochs=10, per_gpu_batch_size=100, learning_rate=0.001, warmup_ratio=0, output_every_n_step=5000, save_every_n_epoch=2, operation='training', use_docid_rank='False', load_ckpt='False', save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', test_file_path='../dataset/test_data_top_300k/', pretrain_model_path='../transformer_models/t5-base', load_ckpt_path='./model/', dataset_script_dir='../data_scripts', dataset_cache_dir='../../negs_tutorial_cache', add_doc_num=319927, max_seq_length=128, max_docid_length=1, use_origin_head='False', num_beams=10, batch_size=200)
Epoch 1/10

start a new running with args: Namespace(epochs=10, per_gpu_batch_size=50, learning_rate=0.001, warmup_ratio=0, output_every_n_step=5000, save_every_n_epoch=2, operation='training', use_docid_rank='False', load_ckpt='False', save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', test_file_path='../dataset/test_data_top_300k/', pretrain_model_path='../transformer_models/t5-base', load_ckpt_path='./model/', dataset_script_dir='../data_scripts', dataset_cache_dir='../../negs_tutorial_cache', add_doc_num=319927, max_seq_length=128, max_docid_length=1, use_origin_head='False', num_beams=10, batch_size=100)
Epoch 1/10

start a new running with args: Namespace(epochs=10, per_gpu_batch_size=1, learning_rate=0.001, warmup_ratio=0, output_every_n_step=5000, save_every_n_epoch=2, operation='training', use_docid_rank='False', load_ckpt='False', save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', test_file_path='../dataset/test_data_top_300k/', pretrain_model_path='../transformer_models/t5-base', load_ckpt_path='./model/', dataset_script_dir='../data_scripts', dataset_cache_dir='../../negs_tutorial_cache', add_doc_num=319927, max_seq_length=128, max_docid_length=1, use_origin_head='False', num_beams=10, batch_size=2)
Epoch 1/10

start a new running with args: Namespace(epochs=10, per_gpu_batch_size=1, learning_rate=0.001, warmup_ratio=0, output_every_n_step=5000, save_every_n_epoch=2, operation='training', use_docid_rank='False', load_ckpt='False', save_path='../outputs/t5_128_10_top_300k_atomic_pretrain/', log_path='../logs/pretrain.t5_128_10.top.300k.atomic.pretrain.log', doc_file_path='../dataset/msmarco-data/msmarco-docs-sents.top.300k.json', docid_path='../dataset/encoded_docid/t5_atomic_top_300k.txt', train_file_path='../dataset/train_data_top_300k/pretrain.t5_128_10.atomic.300k.json', test_file_path='../dataset/test_data_top_300k/', pretrain_model_path='../transformer_models/t5-base', load_ckpt_path='./model/', dataset_script_dir='../data_scripts', dataset_cache_dir='../../negs_tutorial_cache', add_doc_num=319927, max_seq_length=128, max_docid_length=1, use_origin_head='False', num_beams=10, batch_size=2)
Epoch 1/10
